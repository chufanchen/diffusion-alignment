# diffusion-alignment

```
format:
- [title](paper link) [links]
  - author1, author2, and author3...
  - publisher
  - keyword
  - code
  - experiment environments and datasets
```

- [ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation](https://proceedings.neurips.cc/paper_files/paper/2023/hash/33646ef0ed554145eab65f6250fab0c9-Abstract-Conference.html)
  - Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, Yuxiao Dong
  - NeurIPS 2023
  - Keywords: General-purpose text-to-Image human preference RM, Evaluating Text-to-Image Generative Models
  - [Code](https://github.com/THUDM/ImageReward)
  - Experiment Environments and Datasets: COCO, DiffusionDB

- [DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models](https://arxiv.org/abs/2305.16381) [links]
  - Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, Kimin Lee
  - NeurIPS 2023
  - Keywords: Diffusion Models, Reinforcement Learning, Text-to-Image Generation
  - [Code](https://github.com/google-research/google-research/tree/master/dpok)
  - Experiment Environments and Datasets: MS-CoCo, Drawbench
  - LLM Counterpart: DPO → DPOK

- [Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation](https://arxiv.org/abs/2402.04998)
  - Huizhuo Yuan, Zixiang Chen, Kaixuan Ji, Quanquan Gu
  - NeurIPS 2024
  - Keywords: Self-play fine-tuning, Diffusion models, Text-to-image generation
  - [Code](https://github.com/HuizhuoYuan/SPIN-Diffusion)
  - Dataset: Pick-a-Pic
  - LLM Counterpart: Self-Play Fine-Tuning → SPIN-Diffusion

- [A Dense Reward View on Aligning Text-to-Image Diffusion with Preference](https://arxiv.org/abs/2402.08265) 
  - Shentao Yang, Tianqi Chen, Mingyuan Zhou  
  - Publisher: Under review (arXiv preprint, February 2024)  
  - Keyword: Dense reward, Preference alignment, Diffusion models  
  - Code: [official](https://github.com/Shentao-YANG/Dense_Reward_T2I)  

- [Aligning Few-Step Diffusion Models with Dense Reward Difference Learning](https://arxiv.org/abs/2411.11727)
  - Ziyi Zhang, Li Shen, Sen Zhang, Deheng Ye, Yong Luo, Miaojing Shi, Bo Du, Dacheng Tao
  - Publisher: arXiv preprint, November 2024
  - Keyword: Stepwise Diffusion Policy Optimization (SDPO), Dense reward feedback, Few-step diffusion models
  - Code: [official](https://github.com/ZiyiZhang27/sdpo)

- [REBEL: Reinforcement Learning via Regressing Relative Rewards](https://arxiv.org/abs/2404.16767)
  - Zhaolin Gao, Jonathan D. Chang, Wenhao Zhan, Owen Oertell, Gokul Swamy, Kianté Brantley, Thorsten Joachims, J. Andrew Bagnell, Jason D. Lee, Wen Sun
  - NeurIPS 2024
  - Keyword: Policy optimization, Relative rewards, Reinforcement learning
  - Code: [official](https://github.com/ZhaolinGao/REBEL)
  - Dataset: Nectar, UltraFeedback
